{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b532e6-e824-461d-9aa7-cd7996de0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706645cc-c992-4fca-bdb4-645d23a0f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                               text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n"
     ]
    }
   ],
   "source": [
    "# Load the data correctly\n",
    "financial_headlines = pd.read_csv(\n",
    "    \"all_financial_sentiment_data.csv\",\n",
    "    names=[\"sentiment\", \"text\"],  # Ensure column names match your CSV\n",
    "    encoding=\"utf-8\",\n",
    "    encoding_errors=\"replace\",\n",
    "    header=None  # Add this if your CSV has no header row\n",
    ")\n",
    "\n",
    "# Now, set Y to the entire DataFrame (not a list of subsets)\n",
    "Y = financial_headlines.copy()\n",
    "\n",
    "# Verify the structure\n",
    "print(Y.head())  # Should show columns 'sentiment' and 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c615d2d-5a1e-4f5f-ad36-6dc85baba912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c89e1f-67b2-49af-988c-afbbfad0c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  According to Gran , the company has no plans t..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a366c2-fbfe-4ffe-bec8-ea36fbc9c0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c233687d-a875-41aa-81be-50edcc60f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799915ae-cde0-4ad2-b178-d75e8e1c79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22880947-1249-4d49-ab68-fdd11c54c1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55cb51-cb19-4038-a991-f58767e969fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff09e5-6a45-42b9-ba7c-292bb55634fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9f9eb74-d2f6-4aee-863e-9118fd14a670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00         1\n",
      "    positive       1.00      0.85      0.92        47\n",
      "     neutral       0.22      1.00      0.36         2\n",
      "\n",
      "    accuracy                           0.86        50\n",
      "   macro avg       0.74      0.95      0.76        50\n",
      "weighted avg       0.97      0.86      0.90        50\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1  0  0]\n",
      " [ 0  2  0]\n",
      " [ 0  7 40]]\n",
      "Perplexity: 4.09\n",
      "BLEU Score: 0.00\n",
      "ROUGE Scores: {'rouge-1': {'r': 0.86, 'p': 0.86, 'f': 0.8599999957000004}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.86, 'p': 0.86, 'f': 0.8599999957000004}}\n",
      "BERTScore F1: 1.00\n",
      "Faithfulness Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "# Step 1: Set up OpenAI API key\n",
    "openai.api_key = \"key\"\n",
    "\n",
    "# Step 2: Use Y as input and convert it to a DataFrame if necessary\n",
    "if not isinstance(Y, pd.DataFrame):\n",
    "    if isinstance(Y, list):\n",
    "        # Assuming Y is a list of texts and sentiments\n",
    "        if len(Y) > 0 and isinstance(Y[0], str):\n",
    "            # Create a DataFrame with text and sentiment columns\n",
    "            texts = [y for y in Y]\n",
    "            sentiments = [None]*len(Y)  # You need to provide sentiments for all texts\n",
    "            Y = pd.DataFrame({\n",
    "                'text': texts,\n",
    "                'sentiment': sentiments\n",
    "            })\n",
    "        elif len(Y) > 0 and isinstance(Y[0], list) and len(Y[0]) == 2:\n",
    "            # Create a DataFrame with text and sentiment columns\n",
    "            texts = [y[0] for y in Y]\n",
    "            sentiments = [y[1] for y in Y]\n",
    "            Y = pd.DataFrame({\n",
    "                'text': texts,\n",
    "                'sentiment': sentiments\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input format\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type\")\n",
    "\n",
    "# Step 3: Define sentiment classification function\n",
    "def classify_sentiment_gpt4(text):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the sentiment of the following financial news text and classify it as one of the following:\n",
    "    - Positive\n",
    "    - Negative\n",
    "    - Neutral\n",
    "\n",
    "    Text: \"{text}\"\n",
    "    Sentiment:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sentiment classification: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 4: Generate predictions\n",
    "subset_Y = Y.sample(n=min(100, len(Y)), random_state=42)  # Ensure n is not larger than the length of Y\n",
    "subset_Y['predicted_sentiment'] = subset_Y['text'].apply(classify_sentiment_gpt4)\n",
    "subset_Y = subset_Y.dropna(subset=['predicted_sentiment'])\n",
    "\n",
    "true_labels = subset_Y['sentiment'].str.strip().str.lower()\n",
    "predicted_labels = subset_Y['predicted_sentiment'].str.strip().str.lower()\n",
    "\n",
    "# Step 5: Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "unique_labels = list(set(true_labels))\n",
    "report = classification_report(true_labels, predicted_labels, labels=unique_labels)\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Perplexity from googlespaper\n",
    "def calculate_perplexity(true_labels, predicted_labels):\n",
    "    label_to_id = {label: idx for idx, label in enumerate(set(true_labels))}\n",
    "    true_ids = torch.tensor([label_to_id[label] for label in true_labels])\n",
    "    pred_ids = torch.tensor([label_to_id[label] for label in predicted_labels])\n",
    "    logits = torch.randn(len(true_labels), len(label_to_id))\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    loss = loss_fn(logits, true_ids)\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "perplexity = calculate_perplexity(true_labels, predicted_labels)\n",
    "\n",
    "# BLEU Score\n",
    "def calculate_bleu(true_labels, predicted_labels):\n",
    "    bleu_scores = []\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        reference = [true.split()]\n",
    "        candidate = pred.split()\n",
    "        bleu_scores.append(sentence_bleu(reference, candidate))\n",
    "    return sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "bleu_score = calculate_bleu(true_labels, predicted_labels)\n",
    "\n",
    "# ROUGE Score\n",
    "def calculate_rouge(true_labels, predicted_labels):\n",
    "    rouge = Rouge()\n",
    "    return rouge.get_scores(predicted_labels.tolist(), true_labels.tolist(), avg=True)\n",
    "\n",
    "rouge_scores = calculate_rouge(true_labels, predicted_labels)\n",
    "\n",
    "\n",
    "# BERTScore\n",
    "def calculate_bertscore(true_labels, predicted_labels):\n",
    "    P, R, F1 = score(predicted_labels.tolist(), true_labels.tolist(), lang=\"en\")\n",
    "    return F1.mean().item()\n",
    "\n",
    "bertscore_f1 = calculate_bertscore(true_labels, predicted_labels)\n",
    "\n",
    "# Faithfulness fromdeepeval\n",
    "def check_faithfulness(text, predicted_sentiment):\n",
    "    prompt = f\"\"\"\n",
    "    Does the following sentiment prediction align with the tone of the input text?\n",
    "    Text: \"{text}\"\n",
    "    Predicted Sentiment: \"{predicted_sentiment}\"\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content.strip().lower() == \"yes\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during faithfulness check: {e}\")\n",
    "        return False\n",
    "\n",
    "faithfulness_scores = subset_Y.apply(\n",
    "    lambda row: check_faithfulness(row['text'], row['predicted_sentiment']), axis=1\n",
    ")\n",
    "faithfulness_accuracy = faithfulness_scores.mean()\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "print(\"ROUGE Scores:\", rouge_scores)\n",
    "#print(f\"METEOR Score: {meteor_score_value:.2f}\")\n",
    "print(f\"BERTScore F1: {bertscore_f1:.2f}\")\n",
    "print(f\"Faithfulness Accuracy: {faithfulness_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b9508b-0b41-4b7f-8088-53365f24fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## othersmetrics toadd asneecesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f44ab-f1eb-4aef-a5cf-edd2112a6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METEOR Score\n",
    "def calculate_meteor(true_labels, predicted_labels):\n",
    "    meteor_scores = []\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        reference = [true.split()]\n",
    "        candidate = pred.split()\n",
    "        meteor_scores.append(meteor_score(reference, candidate))\n",
    "    return sum(meteor_scores) / len(meteor_scores)\n",
    "\n",
    "meteor_score_value = calculate_meteor(true_labels, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e19edf-80cd-4ccb-8654-6a926fb76b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Y to a CSV file\n",
    "Y.to_csv('SentimentAnalysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb31dd3b-c60f-408d-8d1d-54bb5fccc100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c453b1f-ea1f-44e1-a67e-6a0366fb69d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e35c8-4ea0-4dfe-8d53-58095c10b1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5528a2-15aa-46a3-85ad-846a8d3e800d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69888f4f-8cdc-46a3-b695-30e4ef5c95c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
